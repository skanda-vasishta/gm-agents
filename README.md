# GM Agents: Heuristic-Based, Reward-Oriented Browser Game Playing Agents

## Usage Instructions

To get started with this project, ensure you have Python 3.x installed on your system. Clone the repository to your local machine and install the required dependencies using `pip install -r requirements.txt`. You will need an OpenAI API key, which should be set in your environment variables (for example, by creating a `.env` file with the line `OPENAI_API_KEY=your_key_here`).

To run the main agent, execute `python browse_use/web2.py`. This will launch the browser automation and begin the agent's management of a Basketball GM team. If you wish to train or retrain the reward model on your own feedback data, you can run `python browse_use/train_reward.py` after collecting trade feedback. To test the reward model, use `python browse_use/test_reward.py`.

The agent may prompt you for feedback on trade decisions during operation, and your responses will be logged for future model improvement. For best results, ensure you have a stable internet connection and that all dependencies are properly installed.

# GM Agents: Heuristic-Based, Reward-Oriented Browser Game Playing Agents
The project we present here is a sophisticated, modular system designed to automate and optimize the management of a professional basketball team within the Basketball GM game environment. At its core, this system leverages the latest advances in browser automation, large language models (LLMs), and custom reward modeling to create a context-aware, phase-driven agent capable of making nuanced, high-level decisions that mirror those of a human general manager. The architecture and methodology are informed by both practical engineering and research-driven approaches, as detailed below.

## System Architecture and Technical Stack
The architecture of the system is best understood as a set of interacting modules, each with a clearly defined role in the decision-making pipeline. The central component is the Browser Use Agent, which is both prompt- and context-driven. This agent is responsible for orchestrating the flow of information and actions within the Basketball GM web interface, using Playwright for robust browser automation. The agent is equipped with in-built context, reasoning capabilities, and a suite of custom tools that allow it to interact with the game environment in a human-like manner. Supporting the agent is a reward model for trades, which is trained on a dataset of trade proposals and human evaluations. This model is responsible for providing binary accept/reject decisions on trade proposals, using a fast, interpretable logistic regression pipeline built on TF-IDF vectorization of trade descriptions. The reward model is not static; it is fine-tuned on a growing dataset of labeled examples, allowing it to adapt to new strategies and player valuations over time. The dataset itself is generated through a combination of Playwright automation, DOM parsing, and human-in-the-loop evaluation, ensuring that the model is grounded in realistic, context-rich examples. A critical innovation in this system is the use of GPT-4.1 for state information extraction. Rather than relying solely on structured data, the agent uses OCR and LLM-based parsing to extract nuanced game state information from the browser, such as team ratings, payroll, and phase-specific context. This allows the agent to operate robustly even as the underlying game interface evolves, and to reason about the state in a way that is both flexible and extensible.


## Augmenting Browser Use Agent Capabilities
While the foundational Browser Use agent is designed for generic web navigation and basic interaction, our system significantly extends its capabilities to meet the complex demands of managing a simulated basketball franchise. Out of the box, a Browser Use agent can only perform rudimentary actions such as clicking buttons, following links, and extracting visible text. However, the requirements of Basketball GM—where strategic reasoning, stateful decision-making, and context-sensitive actions are paramount—necessitate a much richer set of functionalities. To bridge this gap, we implemented a suite of custom tools and hooks within the agent, as seen in web2.py. One of the most important augmentations is the integration of state extraction routines that leverage both OCR and LLM-based parsing. For example, the parse_game_state_with_openai and parse_season_state_with_openai functions use Playwright to capture targeted screenshots of the game's UI, which are then passed to GPT-4.1 via the OpenAI API. The LLM is prompted to extract structured information such as team records, ratings, payroll, and the current phase of the season. This approach allows the agent to robustly interpret the game state, even as the UI evolves or presents information in non-standard formats. Another major enhancement is the phase-aware scaffolding, implemented through the PhaseManager class. This component tracks the current phase of the basketball season and manages the number of actions available in each phase. It enables the agent to adapt its behavior dynamically, ensuring that actions are contextually appropriate—such as focusing on trades during the trade deadline or shifting to playoff strategies as the season progresses. The phase manager also coordinates transitions between phases, automating multi-step UI interactions to advance the game state. The agent's action repertoire is further extended through custom controller actions. For instance, the ask_human and ask_llm actions allow the agent to seek guidance from either a human user or an external LLM, injecting expert knowledge or user preferences into the decision loop. This is particularly useful at the start of each phase, where strategic direction can be set interactively. Trade evaluation is another area where the agent's capabilities are augmented far beyond basic browsing. The evaluate_trade_logic function orchestrates a multi-step process: it captures a screenshot of a trade proposal, uses GPT-4.1 to extract and format the trade details, and then passes this information to the reward model for probabilistic evaluation. The agent can then autonomously accept or reject trades based on model output, and even log its decisions and confidence scores for further analysis or retraining. To support continuous learning and improvement, the agent is equipped with mechanisms for collecting user feedback and saving trade data. After each trade decision, the agent can prompt the user for agreement or disagreement, and this feedback is logged alongside the trade details and AI decision. This data is then used to iteratively fine-tune the reward model, closing the loop between agent actions, user oversight, and model improvement. Finally, the agent's main execution loop is structured around asynchronous hooks (state_hook and router_hook) that ensure state is always up-to-date and that actions are taken in a logically consistent order. These hooks coordinate the extraction of state, the evaluation of possible actions, and the transition between phases, providing a robust backbone for the agent's operation. In summary, by layering these custom tools, hooks, and learning mechanisms on top of the basic Browser Use agent, we transform it from a simple web navigator into a sophisticated, context-aware, and continuously improving AI manager. This augmentation is essential for achieving human-level performance in the complex, dynamic environment of Basketball GM.

## Multi-Agent, Heuristic-Driven Design
The agent's decision-making process is organized around the natural phases of a basketball season: Pre-Season, Trade Deadline, and Playoffs. Each phase is managed by a dedicated PhaseManager, which tracks the current phase and the number of actions remaining. This phase-aware scaffolding is crucial for ensuring that the agent's strategies are contextually appropriate; for example, aggressive roster moves may be prioritized at the trade deadline, while more conservative, evaluative strategies are employed during the playoffs. Within each phase, the agent employs a combination of in-built heuristics and external consultation. The heuristics are divided into phase heuristics, which govern high-level strategy (such as when to initiate trades or focus on player development), and game heuristics, which handle more granular decisions (such as evaluating the impact of a specific trade on team chemistry or salary cap). The agent is also capable of consulting external agents—such as additional LLMs or human advisors—when faced with particularly complex or ambiguous situations. A key feature of the system is the "handoff" mechanism between the browser agent and the reward model. During the decision-making phase, the agent gathers and injects state information via GPT-4.1, then, once per phase, hands off trade proposals to the reward model for evaluation. This modular approach allows for clear separation of concerns: the browser agent focuses on context and interaction, while the reward model provides fast, data-driven assessments of trade quality.

## Dataset Generation and Reward Model Training
The process of generating the dataset for the reward model is itself a sophisticated pipeline. Using Playwright automation, the agent creates new trades within the game, extracts and parses trade information from the DOM, and then uses an untrained model to make an initial binary decision. This decision is then evaluated by us, where we provide the final label. Each example—comprising the trade description and its label—is added to the dataset, which is then used to fine-tune the reward model. The reward model is trained using a supervised approach, with each trade proposal treated as a document and labeled as either accepted or rejected. The text is vectorized using TF-IDF, capturing both unigrams and bigrams, and then passed to a logistic regression classifier. This approach is chosen for its speed and interpretability, which are critical given the relatively small size of the manually annotated dataset (approximately 100 examples). While larger models could potentially offer greater accuracy, the current approach strikes a balance between performance and practicality.

## Decision-Making Workflow
The general workflow for each decision-making "step" is as follows. First, the agent gathers and injects state information using GPT-4.1, ensuring that all decisions are grounded in the most up-to-date context. Once per phase, the agent hands off trade proposals to the reward model for evaluation. The agent then executes general decision-making behaviors, which may include consulting external agents or applying additional heuristics. After each decision, the number of decisions made in the current phase is incremented, and the process repeats until the maximum number of decisions for the phase is reached. This workflow ensures that the agent's actions are both contextually aware and data-driven. By limiting the number of decisions per phase, the system mimics the real-world constraints faced by human general managers, who must balance short-term gains against long-term strategy.

## Evaluation and Results
The effectiveness of the system has been evaluated through extensive simulation, with and without the reward model handoff. In headless browser state, the agent was run through 20 simulated game "runs" in each configuration. With the reward model handoff enabled, the agent achieved a team rating of 59.4/100, an average margin of victory of 1.1, and a championship win rate of 13.4%, with an average win percentage of 56.2%. In contrast, without the reward model handoff, the agent's team rating dropped to 50.2/100, with a negative margin of victory, a championship win rate of just 2.4%, and an average win percentage of 36.7%. These results demonstrate the substantial impact of the reward model on the agent's performance, particularly in terms of long-term success metrics.

## Innovations and Future Directions
Several key innovations distinguish this system from prior work. The custom tool equipping of browser agents allows for highly specialized, context-aware interactions with the game environment. The phase- and game-specific heuristics provide a flexible scaffolding for decision-making, enabling the agent to adapt its strategies dynamically as the season progresses. The modular handoff between the browser agent and the reward model ensures that each component can be improved independently, facilitating rapid iteration and experimentation. Looking forward, there are several promising avenues for future development. The reward model could be fine-tuned on a larger, more diverse dataset, or replaced with a more powerful model as more data becomes available. Artificial data augmentation techniques could be used to expand the training set, and the reward modeling approach could be extended to other in-game actions beyond trades. There is also potential for integrating reinforcement learning approaches, allowing the agent to learn optimal strategies through sequential decision-making rather than supervised fine-tuning alone. Additional phase support and dynamic action generation based on real-time needs would further enhance the agent's flexibility and realism.

## Conclusion
In summary, this project represents a comprehensive, state-of-the-art approach to automated sports management in a browser-based simulation environment. By combining browser automation, LLM-driven state extraction, custom reward modeling, and phase-aware heuristics, the system is able to make intelligent, context-sensitive decisions that rival those of human experts. The modular, extensible architecture ensures that the system can continue to evolve as new techniques and data become available, making it a valuable platform for both research and practical application in the field of AI-driven sports management.
